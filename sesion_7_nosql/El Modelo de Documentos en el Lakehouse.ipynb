{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2983c3c-9778-489f-ae19-b7f9cc736015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Título: El Modelo de Documentos en el Lakehouse\n",
    "\n",
    "**Objetivo:** En esta sección, tomaremos nuestros datos de noticias crudos y los transformaremos en una tabla Delta Lake que imita la estructura y flexibilidad de una base de datos documental como MongoDB o Cosmos DB. Aprenderemos a manejar datos semi-estructurados usando tipos complejos y a aprovechar los beneficios del Lakehouse.\n",
    "\n",
    "**Fases del Taller:**\n",
    "1.  **Ingesta a Bronce:** Cargar los datos crudos del CSV a nuestra capa de ingesta.\n",
    "2.  **Transformación a Plata:** Limpiar, estructurar y enriquecer los datos, creando una tabla que se comporte como una colección de documentos.\n",
    "3.  **Consulta y Evolución:** Demostrar cómo consultar y evolucionar el esquema de nuestra tabla \"documental\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39013f0-75b5-45dd-b2b2-13d09283b8be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuración del Entorno\n",
    "\n",
    "# Es una buena práctica definir nuestras variables de configuración al inicio.\n",
    "# Esto hace que el cuaderno sea más fácil de mantener y reutilizar.\n",
    "\n",
    "db_name = \"curso_arquitecturas\"\n",
    "bronze_table_name = \"noticias_bronze\"\n",
    "silver_table_name = \"noticias_silver\"\n",
    "csv_path = \"/Volumes/sesion_5/bronze/raw_files/Noticias.csv\"\n",
    "\n",
    "# Usamos la base de datos que hemos creado a lo largo del curso.\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "\n",
    "print(f\"Base de datos activa: '{db_name}'\")\n",
    "print(f\"Ruta del archivo de noticias: '{csv_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e219385-3511-4f4e-9724-c23b40c92e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 1: Ingesta a la Capa Bronce (Bronze)\n",
    "\n",
    "Recordemos el principio de la capa Bronce: **es un archivo fiel y sin procesar de los datos de origen**.\n",
    "\n",
    "Nuestra primera tarea es tomar el archivo `Noticias.csv` y cargarlo en una tabla Delta sin aplicar ninguna transformación. Esto nos da un punto de partida auditable y nos permite reprocesar los datos en el futuro si las reglas de negocio cambian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9e2294b-1a0b-4b83-a8dd-fa5a6d667c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Leer el archivo CSV y guardarlo como una tabla Delta en la capa Bronce\n",
    "\n",
    "# Leemos el archivo CSV.\n",
    "# - \"header=True\": Usa la primera fila como nombres de columna.\n",
    "# - \"inferSchema=True\": Spark intentará adivinar los tipos de datos. Para la capa Bronce, esto es aceptable.\n",
    "noticias_raw_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_path)\n",
    "\n",
    "# Guardamos el DataFrame como una tabla Delta.\n",
    "# - \"mode(\"overwrite\")\": Si la tabla ya existe, la reemplazará.\n",
    "# - \"saveAsTable(...)\": Guarda los datos y registra la tabla en el catálogo.\n",
    "noticias_raw_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(bronze_table_name)\n",
    "\n",
    "print(f\"Tabla '{bronze_table_name}' creada exitosamente en la capa Bronce.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c8302a-be29-47b1-9696-68b05b078e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verificación Rápida de la Capa Bronce\n",
    "\n",
    "-- Echemos un vistazo a los datos tal como fueron ingeridos.\n",
    "-- Observa que los datos están crudos, con los nombres de columna originales\n",
    "-- y sin ninguna limpieza aplicada.\n",
    "\n",
    "SELECT * FROM noticias_bronze LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ad60de9-fae6-4b5c-ae83-e3523ded28dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 2: Hacia la Capa Plata (Silver) - Limpieza y Estructuración\n",
    "\n",
    "La capa Plata es donde ocurre la magia. Aquí transformamos los datos crudos en un activo de datos confiable y bien estructurado.\n",
    "\n",
    "Para replicar un **modelo de documentos**, no solo limpiaremos los datos, sino que también los reestructuraremos. Nuestro objetivo es agrupar campos relacionados en una sola columna `STRUCT`, creando una entidad cohesiva y fácil de consultar.\n",
    "\n",
    "**Plan de Transformación:**\n",
    "1.  **Renombrar columnas:** Usar nombres claros y consistentes (ej. `Enlaces` -> `enlace`).\n",
    "2.  **Generar un ID único:** Crear una clave primaria (`id_noticia`) para identificar cada documento.\n",
    "3.  **Extraer y crear metadatos:** Extraer la fecha (simulada por ahora) y la etiqueta (`Etiqueta`) para anidarlas en una nueva columna `metadatos`.\n",
    "4.  **Limpieza:** Eliminar filas donde el contenido de la noticia esté vacío.\n",
    "5.  **Tipado de datos:** Asegurar que cada columna tenga el tipo de dato correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc74a4a-7c07-4e3d-8f0f-a5d946786774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar datos de la capa Bronce para iniciar la transformación\n",
    "\n",
    "df_bronze = spark.read.table(bronze_table_name)\n",
    "\n",
    "print(\"Datos de la capa Bronce cargados y listos para transformar.\")\n",
    "df_bronze.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9289e5d-e822-49fd-bf8b-0d12a7e38de8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_bronze.groupBy(\"Etiqueta\").count().orderBy(col(\"Count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc9763d4-7bb7-4c97-a0f3-741df2934292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"archivo\",\n",
    "    \"colombia\",\n",
    "    \"opinion\",\n",
    "    \"deportes\",\n",
    "    \"cultura\",\n",
    "    \"economia\",\n",
    "    \"bogota\",\n",
    "    \"justicia\",\n",
    "    \"mundo\",\n",
    "    \"vida\",\n",
    "    \"politica\",\n",
    "    \"tecnosfera\",\n",
    "    \"salud\",\n",
    "    \"historias-el-tiempo\",\n",
    "    \"contenido-comercial\",\n",
    "    \"mundial\",\n",
    "    \"elecciones\",\n",
    "    \"podcast\"\n",
    "]\n",
    "df_bronze=df_bronze.filter(col(\"Etiqueta\").isin(categories))\n",
    "df_bronze.groupBy(\"Etiqueta\").count().orderBy(col(\"Count\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2af912-ce88-48ea-9474-e16aed2aef9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.groupBy(\"Etiqueta\").count().orderBy(col(\"Count\").desc()).show()# Aplicar las transformaciones de limpieza y estructuración\n",
    "\n",
    "from pyspark.sql.functions import col, sha2, concat_ws, current_timestamp, struct\n",
    "\n",
    "# Aplicamos la lógica de transformación\n",
    "df_silver = df_bronze \\\n",
    "    .withColumnRenamed(\"Enlaces\", \"enlace\") \\\n",
    "    .withColumnRenamed(\"Título\", \"titulo\") \\\n",
    "    .withColumnRenamed(\"Etiqueta\", \"etiqueta\") \\\n",
    "    .withColumn(\"id_noticia\", sha2(concat_ws(\"||\", col(\"titulo\"), col(\"enlace\")), 256)) \\\n",
    "    .withColumn(\"metadatos\", struct(\n",
    "        col(\"etiqueta\"),\n",
    "        current_timestamp().alias(\"fecha_ingesta\") # Simulamos una fecha\n",
    "    )) \\\n",
    "    .select(\n",
    "        \"id_noticia\",\n",
    "        \"enlace\",\n",
    "        \"titulo\",\n",
    "        \"info\",\n",
    "        \"contenido\",\n",
    "        \"metadatos\"\n",
    "    ) \\\n",
    "    .filter(col(\"contenido\").isNotNull())\n",
    "\n",
    "print(\"Transformaciones a la capa Plata completadas.\")\n",
    "df_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "627db13c-3427-453f-b183-f23ac04f9e27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### El Poder de `STRUCT`: Datos Anidados en una Tabla\n",
    "\n",
    "La celda anterior es el corazón de nuestro modelo documental. La función `struct()` nos permitió crear una columna (`metadatos`) que es, en sí misma, una pequeña tabla. Contiene los campos `etiqueta` y `fecha_ingesta`.\n",
    "\n",
    "Esto es análogo a un objeto JSON anidado en un documento de MongoDB. Nos da la capacidad de agrupar información contextual sin necesidad de crear una tabla separada y un `JOIN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46a9d2ce-b533-472c-b3d9-d99e9d002aa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardar la tabla transformada en la Capa Plata\n",
    "\n",
    "df_silver.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(silver_table_name)\n",
    "\n",
    "print(f\"Tabla '{silver_table_name}' creada exitosamente en la capa Plata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16139d0-bcda-48d5-b8fc-903c2f4ff475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Explorando la Tabla Silver: Nuestro \"Documento\" Estructurado\n",
    "\n",
    "-- Ahora, consultemos nuestra nueva tabla.\n",
    "-- Fíjate en la columna `metadatos`: es una estructura compleja.\n",
    "-- ¡Hemos creado una colección de documentos dentro de nuestro Lakehouse!\n",
    "\n",
    "SELECT * FROM noticias_silver LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb08667-bb94-42a4-9547-b7141ef31ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql DESCRIBE HISTORY noticias_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb2990c-9774-47f7-a911-f4f6622a9e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 3: Consultando y Evolucionando el Modelo\n",
    "\n",
    "Ahora que tenemos nuestra tabla `noticias_silver` con datos anidados, podemos explotar su flexibilidad.\n",
    "\n",
    "#### Consultando Campos Anidados\n",
    "Una de las grandes ventajas de Spark SQL es que puede consultar campos dentro de un `STRUCT` de manera muy intuitiva, usando la **notación de punto**, igual que harías en muchos lenguajes de programación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26700a7b-beb0-4ee5-8917-2083f6b5cb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Usando la notación de punto para acceder a campos dentro del STRUCT\n",
    "\n",
    "SELECT\n",
    "  id_noticia,\n",
    "  titulo,\n",
    "  metadatos.etiqueta AS categoria, -- Accedemos al campo 'etiqueta'\n",
    "  metadatos.fecha_ingesta -- Accedemos al campo 'fecha_ingesta'\n",
    "FROM noticias_silver\n",
    "WHERE metadatos.etiqueta = 'colombia'\n",
    "\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "485997e5-fb54-49c3-a221-40d23c6aa2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8735142320624234,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "El Modelo de Documentos en el Lakehouse",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
