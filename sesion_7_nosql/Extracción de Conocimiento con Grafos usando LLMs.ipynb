{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65252ea2-4900-4cf5-9581-c06909d0be06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cuaderno: Extracción de Conocimiento con Grafos usando LLMs\n",
    "\n",
    "**Objetivo:** Transformar el texto no estructurado de las noticias en un grafo de conocimiento estructurado. Usaremos un Modelo de Lenguaje Grande (LLM) disponible en nuestro endpoint, **`databricks-gemma-3-12b`**, para identificar automáticamente las entidades y sus relaciones.\n",
    "\n",
    "**Fases del Taller:**\n",
    "1.  **Diseñar un Prompt:** Crear la instrucción perfecta para que el LLM actúe como un extractor de conocimiento.\n",
    "2.  **Aplicar el Modelo a Escala:** Usar la función `ai_query` para ejecutar el LLM sobre nuestra tabla `noticias_silver`.\n",
    "3.  **Modelar el Grafo:** Parsear la salida del LLM y almacenarla en tablas Delta de nodos (`entidades_gold`) y aristas (`relaciones_gold`).\n",
    "4.  **Analizar el Grafo:** Utilizar PySpark puro para realizar consultas sobre nuestra nueva red de conocimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d1ab713-7d03-4628-b83e-57b79f3933be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 1: El Arte del Prompt Engineering\n",
    "\n",
    "La clave para que un LLM realice una tarea específica es darle una **instrucción clara y precisa (un \"prompt\")**.\n",
    "\n",
    "Le pediremos al modelo `databricks-gemma-3-12b` que lea un artículo y nos devuelva una estructura que podamos procesar fácilmente: un **formato JSON**.\n",
    "\n",
    "Nuestro prompt le pedirá al modelo dos cosas:\n",
    "1.  Una lista de `entidades` con su nombre y tipo (PERSONA, ORGANIZACION, LUGAR).\n",
    "2.  Una lista de `relaciones` en formato de triplete: `[entidad_origen, tipo_de_relacion, entidad_destino]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb717da-04f0-4a9e-99af-da4466b738c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Configuración Inicial\n",
    "\n",
    "-- Nos aseguramos de estar en la base de datos correcta.\n",
    "USE curso_arquitecturas;\n",
    "\n",
    "-- Damos un vistazo rápido a la tabla silver que creamos en la sección anterior.\n",
    "-- La columna 'contenido' será la entrada para nuestro LLM.\n",
    "SELECT titulo, contenido FROM noticias_silver LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41f24492-e74a-4cbb-9e1e-241f82eefd2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 2: Usando `ai_query` para Interactuar con el Modelo Gemma\n",
    "\n",
    "Ahora que has confirmado que el endpoint `databricks-gemma-3-12b` está disponible, podemos crear una función SQL que lo invoque con nuestro prompt. Esto abstrae la complejidad y hace que nuestro código principal sea más legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489bfc4d-dc90-4f17-b428-67822f762d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Creación de una Función SQL para Extraer el Grafo con Gemma\n",
    "\n",
    "-- Esta función toma el texto de una noticia como entrada y le pide al LLM\n",
    "-- que devuelva las entidades y relaciones en formato JSON.\n",
    "\n",
    "CREATE OR REPLACE FUNCTION EXTRACT_GRAPH_JSON(content STRING)\n",
    "RETURNS STRING\n",
    "RETURN AI_QUERY(\n",
    "  'databricks-gemma-3-12b', \n",
    "  CONCAT(\n",
    "    'Eres un experto analista de inteligencia que extrae conocimiento de textos. ',\n",
    "    'Analiza la siguiente noticia y extrae las principales entidades y sus relaciones. ',\n",
    "    'Responde únicamente con un objeto JSON que contenga dos claves: \"entidades\" y \"relaciones\". ',\n",
    "    'Para las entidades, usa los tipos: PERSONA, ORGANIZACION, LUGAR. ',\n",
    "    'Para las relaciones, usa el formato [sujeto, predicado, objeto]. ',\n",
    "    'Noticia: ', content\n",
    "  )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2efdf38e-4fd4-4561-b374-222a35d67bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Creación de una Función SQL para Extraer el Grafo con Gemma\n",
    "\n",
    "-- Esta función toma el texto de una noticia como entrada y le pide al LLM\n",
    "-- que devuelva las entidades y relaciones en formato JSON. databricks-gpt-oss-120b\n",
    "\n",
    "CREATE OR REPLACE FUNCTION EXTRACT_GRAPH_JSON_GPT(content STRING)\n",
    "RETURNS STRING\n",
    "RETURN AI_QUERY(\n",
    "  'databricks-gpt-oss-120b', \n",
    "  CONCAT(\n",
    "    'Eres un experto analista de inteligencia que extrae conocimiento de textos. ',\n",
    "    'Analiza la siguiente noticia y extrae las principales entidades y sus relaciones. ',\n",
    "    'Responde únicamente con un objeto JSON que contenga dos claves: \"entidades\" y \"relaciones\". ',\n",
    "    'Para las entidades, usa los tipos: PERSONA, ORGANIZACION, LUGAR. ',\n",
    "    'Para las relaciones, usa el formato [sujeto, predicado, objeto]. ',\n",
    "    'Noticia: ', content\n",
    "  )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8126ff7b-320c-4e16-a86a-7ad1474752fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "EXTRACT_GRAPH_JSON(\"En el bosque de la china Xi JINPING se perdio y como GUSTAVO estaba perdido se encontraron los dos\")\n",
    "AS grafo_extraido;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "583a309a-1be7-471c-9211-54ad0e5225f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "EXTRACT_GRAPH_JSON_GPT(\"En el bosque de la china Xi JINPING se perdio y como GUSTAVO estaba perdio se encontraron los dos\")\n",
    "AS grafo_extraido;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a1ad4ab-ba06-4579-9620-c8984c8d07c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 3: Aplicando el LLM sobre un Conjunto de Noticias\n",
    "\n",
    "Ahora que tenemos nuestra función `EXTRACT_GRAPH_JSON`, podemos aplicarla a la columna `contenido`. Para este taller, limitaremos el proceso a una pequeña muestra de 5 noticias para demostrar el concepto de forma rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15b14d1-7ba5-47b6-a7a8-a4c690fe2358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Ejecutamos el LLM sobre 5 noticias y guardamos la salida JSON en una tabla temporal\n",
    "\n",
    "CREATE OR REPLACE TEMP VIEW noticias_con_grafo_json AS\n",
    "SELECT\n",
    "  id_noticia,\n",
    "  titulo,\n",
    "  EXTRACT_GRAPH_JSON(contenido) AS grafo_json\n",
    "FROM curso_arquitecturas.noticias_silver\n",
    "LIMIT 50;\n",
    "\n",
    "-- Revisemos la salida cruda del LLM\n",
    "SELECT titulo, grafo_json FROM noticias_con_grafo_json;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6efbdf2-1ec4-4671-825c-c2db9496c399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW noticias_con_grafo_json AS\n",
    "SELECT\n",
    "  id_noticia,\n",
    "  titulo,\n",
    "  EXTRACT_GRAPH_JSON_GPT(contenido) AS grafo_json\n",
    "FROM curso_arquitecturas.noticias_silver\n",
    "LIMIT 50;\n",
    "\n",
    "-- Revisemos la salida cruda del LLM\n",
    "SELECT titulo, grafo_json FROM noticias_con_grafo_json;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "616e76ac-baca-4489-9f70-64c9f4079728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 4: Parseando el JSON para Estructurar el Grafo\n",
    "\n",
    "La salida del LLM es una cadena de texto en formato JSON. Nuestro siguiente paso es \"desempacar\" (parsear) este JSON y cargarlo en nuestras tablas finales de la capa Oro:\n",
    "* `entidades_gold` (para los nodos)\n",
    "* `relaciones_gold` (para las aristas)\n",
    "\n",
    "Usaremos las funciones de Spark SQL `from_json` y `explode` para esta tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9142a25-a9a9-4583-999c-484c3ae50aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Creación de las Tablas Finales en la Capa Oro\n",
    "\n",
    "CREATE OR REPLACE TABLE entidades_gold (\n",
    "  id_noticia STRING,\n",
    "  nombre_entidad STRING,\n",
    "  tipo_entidad STRING\n",
    ");\n",
    "\n",
    "CREATE OR REPLACE TABLE relaciones_gold (\n",
    "  id_noticia STRING,\n",
    "  entidad_origen STRING,\n",
    "  relacion STRING,\n",
    "  entidad_destino STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a234c7-6bf7-40ca-a72c-ffbc6e56622d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Parsear y Cargar las ENTIDADES (Nodos) - VERSIÓN CORREGIDA FINAL\n",
    "\n",
    "-- Añadimos un paso de limpieza (clean_json) usando regexp_replace para\n",
    "-- eliminar los ```json y ``` que el LLM añade a la salida.\n",
    "\n",
    "INSERT INTO entidades_gold\n",
    "WITH cleaned_json AS (\n",
    "  SELECT\n",
    "    id_noticia,\n",
    "    regexp_replace(grafo_json, '(^```json\\\\n|\\\\n```$)', '') AS json_limpio\n",
    "  FROM noticias_con_grafo_json\n",
    "  WHERE grafo_json IS NOT NULL\n",
    "),\n",
    "parsed_json AS (\n",
    "  SELECT\n",
    "    id_noticia,\n",
    "    from_json(json_limpio, 'STRUCT<entidades: ARRAY<STRUCT<tipo: STRING, nombre: STRING>>, relaciones: ARRAY<ARRAY<STRING>>>') AS data\n",
    "  FROM cleaned_json\n",
    ")\n",
    "SELECT\n",
    "  id_noticia,\n",
    "  entidad.nombre AS nombre_entidad,\n",
    "  entidad.tipo AS tipo_entidad\n",
    "FROM parsed_json\n",
    "LATERAL VIEW EXPLODE(data.entidades) AS entidad;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f4ee56-3370-4ddb-a8bf-a5894a84ee95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM entidades_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691dbca3-229a-4e3d-b713-8bcbd9063a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Parsear y Cargar las RELACIONES (Aristas) - VERSIÓN CORREGIDA Y ROBUSTA\n",
    "\n",
    "-- Aplicamos la misma lógica de limpieza aquí para asegurar la consistencia\n",
    "-- y robustez del pipeline.\n",
    "\n",
    "INSERT INTO relaciones_gold\n",
    "WITH cleaned_json AS (\n",
    "  SELECT\n",
    "    id_noticia,\n",
    "    regexp_replace(grafo_json, '(^```json\\\\n|\\\\n```$)', '') AS json_limpio\n",
    "  FROM noticias_con_grafo_json\n",
    "  WHERE grafo_json IS NOT NULL\n",
    "),\n",
    "parsed_json AS (\n",
    "  SELECT\n",
    "    id_noticia,\n",
    "    from_json(json_limpio, 'STRUCT<entidades: ARRAY<STRUCT<tipo: STRING, nombre: STRING>>, relaciones: ARRAY<ARRAY<STRING>>>') AS data\n",
    "  FROM cleaned_json\n",
    ")\n",
    "SELECT\n",
    "  id_noticia,\n",
    "  get(relacion, 0) AS entidad_origen, -- Acceso seguro al primer elemento\n",
    "  get(relacion, 1) AS relacion,       -- Acceso seguro al segundo elemento\n",
    "  get(relacion, 2) AS entidad_destino -- Acceso seguro al tercer elemento (devolverá NULL si no existe)\n",
    "FROM parsed_json\n",
    "LATERAL VIEW EXPLODE(data.relaciones) AS relacion;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6d0ffe-b8bd-4faf-ba5e-69779c004bab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verificación del Grafo Estructurado: Aristas\n",
    "\n",
    "SELECT * FROM relaciones_gold;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37911ebb-cf0a-4fd3-8285-0917e7b3b3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 5: Análisis de la Red con PySpark Puro\n",
    "\n",
    "Ahora que el grafo está modelado en tablas Delta, usaremos PySpark para analizarlo. Calcularemos los \"grados\" de los nodos para identificar las entidades más importantes o centrales en nuestra pequeña red de noticias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94180742-423e-4116-8391-1430f53ba3ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Análisis 1: Calculando los Grados de los Nodos con PySpark\n",
    "\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# Cargamos la tabla de aristas\n",
    "aristas_df = spark.table(\"relaciones_gold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c9a152-2ef8-47e4-8024-479f928bb1e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Para calcular el grado, contamos cuántas veces aparece una entidad\n",
    "# tanto en la columna de origen como en la de destino.\n",
    "endpoints_df = aristas_df.select(col(\"entidad_origen\").alias(\"entidad\")) \\\n",
    "    .unionAll(aristas_df.select(col(\"entidad_destino\").alias(\"entidad\")))\n",
    "\n",
    "endpoints_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8b2b8fb-e484-4bb7-b6c9-970e3ca3750f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agrupamos por entidad y contamos las ocurrencias.\n",
    "grados_df = endpoints_df.groupBy(\"entidad\").count() \\\n",
    "    .withColumnRenamed(\"count\", \"degree\")\n",
    "\n",
    "grados_df.orderBy(desc(\"degree\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "856bc209-e590-4c3d-8df3-f69db8dbff67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Mostramos las entidades más conectadas.\n",
    "print(\"Entidades más centrales (con más conexiones) calculadas con PySpark:\")\n",
    "display(grados_df.orderBy(desc(\"degree\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c56dddf-5671-49fc-bd75-36586058fde6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paso 6: Visualización con `networkx`\n",
    "\n",
    "Finalmente, aplicamos el patrón híbrido: usamos PySpark para el cálculo distribuido y ahora traemos el resultado (que ya es pequeño) al driver para visualizarlo con `networkx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605990c4-959a-4dda-a1fd-926e796efc45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Traemos el subconjunto de datos a pandas para usar con networkx\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Recolectamos los datos (ya son pequeños) al driver\n",
    "nodos_pd = spark.table(\"curso_arquitecturas.entidades_gold\").select(\"nombre_entidad\").distinct().toPandas()\n",
    "aristas_pd = spark.table(\"curso_arquitecturas.relaciones_gold\").toPandas()\n",
    "\n",
    "nodos_pd=nodos_pd.iloc[:20]\n",
    "aristas_pd=aristas_pd.iloc[:100]\n",
    "# Creamos el grafo con networkx\n",
    "G = nx.from_pandas_edgelist(aristas_pd.dropna(subset=[\"entidad_origen\", \"entidad_destino\"]), \"entidad_origen\", \"entidad_destino\")\n",
    "\n",
    "# Visualizamos el grafo\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, k=0.8, iterations=50)\n",
    "nx.draw(G, pos, with_labels=True, node_size=1000, node_color='skyblue', font_size=9, font_weight='bold', width=1.5, edge_color='gray')\n",
    "plt.title(\"Red de Conocimiento Extraída de las Noticias\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4316526-151c-4cda-88cf-62fe0d7706ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Conclusión de la Sección\n",
    "\n",
    "¡Felicidades! Has completado un flujo de trabajo de IA de vanguardia:\n",
    "\n",
    "1.  **Identificaste** un recurso de cómputo de IA disponible (`databricks-gemma-3-12b`).\n",
    "2.  **Diseñaste un prompt** para instruir al modelo a realizar una tarea de extracción estructurada.\n",
    "3.  **Ejecutaste el LLM** a través de SQL para convertir texto no estructurado en JSON.\n",
    "4.  **Parseaste y modelaste** la salida en un grafo con nodos y aristas en la capa Oro.\n",
    "5.  **Analizaste y visualizaste** la red resultante.\n",
    "\n",
    "Este proceso es la base de sistemas de inteligencia, análisis de riesgo y descubrimiento de información, demostrando cómo una arquitectura Lakehouse moderna puede ir mucho más allá del BI tradicional."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "networkx"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8735142320624287,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Extracción de Conocimiento con Grafos usando LLMs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
